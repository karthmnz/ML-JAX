# -*- coding: utf-8 -*-
"""Copy of ECE1513H - Assignment 4 boilerplate

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C2Wkv2rn1zAe1mnrSQHTNmRVZg0TfbxJ

Let's first get the imports out of the way.
"""

import array
import gzip
import itertools
import numpy
import numpy.random as npr
import os
import struct
import time
from os import path
import urllib.request
import matplotlib.pyplot as plt


import jax.numpy as np
from jax.api import jit, grad
from jax.config import config
from jax.scipy.special import logsumexp
from jax import random

"""The following cell contains boilerplate code to download and load MNIST data."""

_DATA = "/tmp/"

def _download(url, filename):
  """Download a url to a file in the JAX data temp directory."""
  if not path.exists(_DATA):
    os.makedirs(_DATA)
  out_file = path.join(_DATA, filename)
  if not path.isfile(out_file):
    urllib.request.urlretrieve(url, out_file)
    print("downloaded {} to {}".format(url, _DATA))


def _partial_flatten(x):
  """Flatten all but the first dimension of an ndarray."""
  return numpy.reshape(x, (x.shape[0], -1))


def _one_hot(x, k, dtype=numpy.float32):
  """Create a one-hot encoding of x of size k."""
  return numpy.array(x[:, None] == numpy.arange(k), dtype)


def mnist_raw():
  """Download and parse the raw MNIST dataset."""
  # CVDF mirror of http://yann.lecun.com/exdb/mnist/
  base_url = "https://storage.googleapis.com/cvdf-datasets/mnist/"

  def parse_labels(filename):
    with gzip.open(filename, "rb") as fh:
      _ = struct.unpack(">II", fh.read(8))
      return numpy.array(array.array("B", fh.read()), dtype=numpy.uint8)

  def parse_images(filename):
    with gzip.open(filename, "rb") as fh:
      _, num_data, rows, cols = struct.unpack(">IIII", fh.read(16))
      return numpy.array(array.array("B", fh.read()),
                      dtype=numpy.uint8).reshape(num_data, rows, cols)

  for filename in ["train-images-idx3-ubyte.gz", "train-labels-idx1-ubyte.gz",
                   "t10k-images-idx3-ubyte.gz", "t10k-labels-idx1-ubyte.gz"]:
    _download(base_url + filename, filename)

  train_images = parse_images(path.join(_DATA, "train-images-idx3-ubyte.gz"))
  train_labels = parse_labels(path.join(_DATA, "train-labels-idx1-ubyte.gz"))
  test_images = parse_images(path.join(_DATA, "t10k-images-idx3-ubyte.gz"))
  test_labels = parse_labels(path.join(_DATA, "t10k-labels-idx1-ubyte.gz"))

  return train_images, train_labels, test_images, test_labels


def mnist(create_outliers=False): #changed back to false for question 2
  """Download, parse and process MNIST data to unit scale and one-hot labels."""
  train_images, train_labels, test_images, test_labels = mnist_raw()

  train_images = _partial_flatten(train_images) / numpy.float32(255.)
  test_images = _partial_flatten(test_images) / numpy.float32(255.)
  train_labels = _one_hot(train_labels, 10)
  test_labels = _one_hot(test_labels, 10)

  if create_outliers:
    mum_outliers = 30000
    perm = numpy.random.RandomState(0).permutation(mum_outliers)
    train_images[:mum_outliers] = train_images[:mum_outliers][perm]

  return train_images, train_labels, test_images, test_labels

def shape_as_image(images, labels, dummy_dim=False):
  target_shape = (-1, 1, 28, 28, 1) if dummy_dim else (-1, 28, 28, 1)
  return np.reshape(images, target_shape), labels

train_images, train_labels, test_images, test_labels = mnist(create_outliers=False)
num_train = train_images.shape[0]

"""# **Problem 1**

This function computes the output of a fully-connected neural network (i.e., multilayer perceptron) by iterating over all of its layers and:

1. taking the `activations` of the previous layer (or the input itself for the first hidden layer) to compute the `outputs` of a linear classifier. Recall the lectures: `outputs` is what we wrote $z=w\cdot x + b$ where $x$ is the input to the linear classifier. 
2. applying a non-linear activation. Here we will use $tanh$.

Complete the following cell to compute `outputs` and `activations`.
"""

def predict(params, inputs):
  activations = inputs
  for w, b in params[:-1]:
    outputs = np.dot(activations, w) + b
    activations = np.tanh(outputs)

  final_w, final_b = params[-1]
  logits = np.dot(activations, final_w) + final_b
  return logits - logsumexp(logits, axis=1, keepdims=True)

"""The following cell computes the loss of our model. Here we are using cross-entropy combined with a softmax but the implementation uses the `LogSumExp` trick for numerical stability. This is why our previous function `predict` returns the logits to which we substract the `logsumexp` of logits. We discussed this in class but you can read more about it [here](https://blog.feedly.com/tricks-of-the-trade-logsumexp/).

Complete the return line. Recall that the loss is defined as :
$$ l(X, Y) = -\frac{1}{n} \sum_{i\in 1..n}  \sum_{j\in 1.. K}y_j^{(i)} \log(f_j(x^{(i)})) = -\frac{1}{n} \sum_{i\in 1..n}  \sum_{j\in 1.. K}y_j^{(i)} \log\left(\frac{z_j^{(i)}}{\sum_{k\in 1..K}z_k^{(i)}}\right) $$
where $X$ is a matrix containing a batch of $n$ training inputs, and $Y$ a matrix containing a batch of one-hot encoded labels defined over $K$ labels. Here $z_j^{(i)}$ is the logits (i.e., input to the softmax) of the model on the example $i$ of our batch of training examples $X$.
"""

def loss(params, batch):
  inputs, targets = batch
  preds = predict(params, inputs)
  temp = targets * preds
  sum_ = np.sum(temp, axis=1)
  return -np.sum(sum_)/inputs.shape[0]

"""The following cell defines the accuracy of our model and how to initialize its parameters."""

def accuracy(params, batch):
  inputs, targets = batch
  target_class = np.argmax(targets, axis=1)
  predicted_class = np.argmax(predict(params, inputs), axis=1)
  return np.mean(predicted_class == target_class)

def init_random_params(layer_sizes, rng=npr.RandomState(0)):
  scale = 0.1
  return [(scale * rng.randn(m, n), scale * rng.randn(n))
          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]

"""The following line defines our architecture with the number of neurons contained in each fully-connected layer (the first layer has 784 neurons because MNIST images are 28*28=784 pixels and the last layer has 10 neurons because MNIST has 10 classes)"""

layer_sizes = [784, 5000, 5000, 5000, 2048, 2048, 2048, 10] #leaving this in the setting for q1.7, but please set the create_outliers = True for better overfitting result

"""The following cell creates a Python generator for our dataset. It outputs one batch of $n$ training examples at a time."""

batch_size = 392
num_complete_batches, leftover = divmod(num_train, batch_size)
num_batches = num_complete_batches + bool(leftover)

def data_stream():
  rng = npr.RandomState(0)
  while True:
    perm = rng.permutation(num_train)
    for i in range(num_batches):
      batch_idx = perm[i * batch_size:(i + 1) * batch_size]
      yield train_images[batch_idx], train_labels[batch_idx]
batches = data_stream()

"""We are now ready to define our optimizer. Here we use mini-batch stochastic gradient descent. Complete `<w UPDATE RULE>` and `<b UPDATE RULE>` using the update rule we saw in class. Recall that `dw` is the partial derivative of the `loss` with respect to `w` and `learning_rate` is the learning rate of gradient descent."""

# lr = [5, 3, 2.5, 2.15, 1.8, 1.5, 1.3, 1, 0.9, 0.7, 0.5, 0.3, 0.1, 0.09, 0.07, 0.05, 0.03, 0.01, 0.009, 0.007, 0.005, 0.003, 0.001, 0.0009, 0.0007, 0.0005, 0.0003, 0.0001, 0.00009, 0.00007, 0.00005, 0.00003, 0.00001, 0.000009, 0.000007, 0.000005, 0.000003, 0.000001]
# Previously, exhaustively tried different learning rates in different logarathmic scales and picked out the best one for the questions.

lr = [0.001]

@jit
def update(params, batch, learning_rate):
  grads = []
  grads = grad(loss)(params, batch)
  return [(w - learning_rate * dw, b - learning_rate * db)
          for (w, b), (dw, db) in zip(params, grads)]

"""This is now the proper training loop for our fully-connected neural network."""

train_acc_ = []
test_acc_ = []
for lr_ in range(len(lr)):
  learning_rate = lr[lr_]
  print("*******************************\nGrid Search : lr {}\n*******************************".format(lr[lr_]))
  num_epochs = 10
  params = init_random_params(layer_sizes)
  for epoch in range(num_epochs):
    start_time = time.time()
    for _ in range(num_batches):
      params = update(params, next(batches), learning_rate)
    epoch_time = time.time() - start_time

    train_acc = accuracy(params, (train_images, train_labels))
    test_acc = accuracy(params, (test_images, test_labels))

    train_acc_.append(train_acc*100)
    test_acc_.append(test_acc*100)
    
    print("Epoch {} in {:0.2f} sec".format(epoch, epoch_time))
    print("Training set accuracy {}".format(train_acc*100))
    print("Test set accuracy {}".format(test_acc*100))

test_acc_ = np.array(test_acc_).reshape(-1,num_epochs)
train_acc_ = np.array(train_acc_).reshape(-1,num_epochs)


for i in range(len(lr)):
    x = [j+1 for j in range(num_epochs)]
    plt.figure()
    plt.plot(x, train_acc_[i], label = 'train_acc')
    plt.plot(x, test_acc_[i], label = 'test_acc')
    plt.title('leanring rate: {}'.format(lr[i]))
    plt.legend()
    plt.show()

"""# **Problem 2**

Before we get started, we need to import two small libraries that contain boilerplate code for common neural network layer types and for optimizers like mini-batch SGD.
"""

from jax.experimental import optimizers
from jax.experimental import stax

"""Here is a fully-connected neural network architecture, like the one of Problem 1, but this time defined with `stax`"""

init_random_params, predict = stax.serial(
    
    stax.Conv(out_chan = 6, filter_shape = (3, 3), padding = 'VALID'), 
    stax.MaxPool(window_shape= (2, 2)),
    stax.BatchNorm(),
    stax.Relu,

    stax.Conv(out_chan = 16, filter_shape = (3, 3), padding = 'VALID'),
    stax.MaxPool(window_shape= (2, 2)),
    stax.BatchNorm(),
    stax.Relu,

    stax.Conv(out_chan = 32, filter_shape = (3, 3), padding = 'VALID'),
    stax.MaxPool(window_shape= (2, 2)),
    stax.BatchNorm(),
    stax.Relu,
    
    stax.Conv(out_chan = 32, filter_shape = (3, 3), padding = 'VALID'),
    stax.MaxPool(window_shape= (2, 2)),
    stax.BatchNorm(),
    stax.Relu,

    stax.Flatten,
    stax.Dense(1024),
    stax.Relu,
    stax.Dense(128),
    stax.Relu,
    stax.Dense(10)
)

"""We redefine the cross-entropy loss for this model. As done in Problem 1, complete the return line below (it's identical)."""

def loss(params, batch):
  inputs, targets = batch
  logits = predict(params, inputs)
  preds  = stax.logsoftmax(logits)
  temp = targets * preds
  sum_ = np.sum(temp, axis=1)
  return -np.sum(sum_)/inputs.shape[0]

"""Next, we define the mini-batch SGD optimizer, this time with the optimizers library in JAX."""

# learning_rate__ = [1, 0.9, 0.7, 0.5, 0.3, 0.15, 0.09, 0.07, 0.05, 0.03, 0.01, 0.009, 0.007, 0.005, 0.003, 0.001, 0.0009, 0.0007, 0.0005, 0.0003, 0.0001, 0.00009, 0.00007, 0.00005, 0.00003, 0.00001, 0.000009, 0.000007, 0.000005, 0.000003, 0.000001]
# Previously, exhaustively tried different learning rates in different logarathmic scales and picked out the best one for the questions.
# num_batches_ = [12,25,49,98,128,145,196,392]
# Also tried different bs

learning_rate__ = [0.3, 0.05, 0.005]
num_batches_ = [128, 64]
@jit
def update(_, i, opt_state, batch):
  grads = []
  params = get_params(opt_state)
  return opt_update(i, grad(loss)(params, batch), opt_state)

"""The next cell contains our training loop, very similar to Problem 1."""

num_runs = 5
num_epochs = 15
tes_acc = []
tes_loss = []
plt_mean = []
plt_std = []

for lr in range(len(learning_rate__)):
  learning_rate = learning_rate__[lr]
  opt_init, opt_update, get_params = optimizers.sgd(learning_rate)
  key = random.PRNGKey(123)
  _, init_params = init_random_params(key, (-1, 28, 28, 1))
  opt_state = opt_init(init_params)
  itercount = itertools.count()

  for i in range(len(num_batches_)):
    batch_size = num_batches_[i]
    num_complete_batches, leftover = divmod(num_train, batch_size)
    num_batches = num_complete_batches + bool(leftover)
    batches = data_stream()
        
    print("\n*******************************\nGrid Search : lr {} and bs {}\n*******************************\n".format(learning_rate, batch_size))
        
    mean_over_run = [] #each run
    std_over_run = []

    mean_over_epochs = [] #each epoch
    std_over_epochs = []

    for each_run in range(num_runs):
      print("run {}".format(each_run+1))

      test_epoch = []  
      for epoch in range(1, num_epochs + 1):
        
        
        for _ in range(num_batches):
          opt_state = update(key, next(itercount), opt_state, shape_as_image(*next(batches)))

        
        params = get_params(opt_state)
        test_acc = accuracy(params, shape_as_image(test_images, test_labels))
        test_loss = loss(params, shape_as_image(test_images, test_labels))
        print('Test set loss, accuracy (%): ({:.2f}, {:.2f})'.format(test_loss, test_acc))
        
        tes_acc.append(test_acc)
        tes_loss.append(test_loss)

        test_epoch.append(test_acc)

      mean = np.array(test_epoch).mean()
      mean_over_epochs.append(mean)
      std = np.array(test_epoch).std()
      std_over_epochs.append(std)
      print("epoch mean {}".format(mean))
      print("epoch std {}".format(std))
    run_mean = np.array(mean_over_epochs).mean()
    run_std = np.array(std_over_epochs).std()

    mean_over_run.append(run_mean) 
    std_over_run.append(run_std)

    plt_mean.append(mean_over_run)
    plt_std.append(std_over_run)

    for o in range(1):
      print("\nMean over 5 runs : {}".format(mean_over_run[o]))
      print("Std over 5 runs : {}\n".format(std_over_run[o]))
      plt_mean.append(mean_over_run[o])
      plt_std.append(std_over_run[o])

